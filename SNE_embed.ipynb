{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SNE_embed.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJWlj/aQUXg5lU0+0coSdF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshi7788/GEM/blob/master/SNE_embed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jGf_8ASQ6pM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "d82a4088-5b74-40ad-db5e-3091a926346f"
      },
      "source": [
        "%tensorflow_version 1.2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `1.2.1`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dvhiXPuRcCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4f591eb2-9f15-4708-bc4c-c3c2f295b11c"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsxIiIIEUgY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHuvr4NqXiWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from absl import flags\n",
        "\n",
        "for name in list(flags.FLAGS):\n",
        "  delattr(flags.FLAGS, name)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DRJL0HmR3x_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "from itertools import tee\n",
        "from six.moves import xrange\n",
        "import numpy as np\n",
        "import sys\n",
        "import logging\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "import logging.config\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)\n",
        "\n",
        "flags = tf.app.flags\n",
        "\n",
        "flags.DEFINE_string(\"save_path\", 'path_1.txt', \"Directory to write the model and training sammaries.\")\n",
        "flags.DEFINE_string(\"train_data\", 'wiki_edit.txt', \"Training text file.\")\n",
        "flags.DEFINE_string(\"label_data\", 'wiki_usr_labels.txt', \"Nodes labels text file.\")\n",
        "flags.DEFINE_string(\"walks_data\", 'wiki_edit_num_40.walk', \"Random walks on data\")\n",
        "flags.DEFINE_integer(\"embedding_size\", 100, \"The embedding dimension size.\")\n",
        "flags.DEFINE_integer(\"samples_to_train\", 25, \"Number of samples to train(*Million).\")\n",
        "flags.DEFINE_float(\"learning_rate\", 0.025, \"Initial learning rate.\")\n",
        "flags.DEFINE_integer(\"num_sampled\", 512, \"The number of classes to randomly sample per batch.\")\n",
        "flags.DEFINE_integer(\"context_size\", 3, \"The number of context nodes .\")\n",
        "flags.DEFINE_integer(\"batch_size\", 50, \"Number of training examples processed per step.\")\n",
        "flags.DEFINE_boolean(\"is_train\", True, \"Train or restore\")\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "\n",
        "def pairwise(iterable):\n",
        "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
        "    a, b = tee(iterable)\n",
        "    next(b, None)\n",
        "    return zip(a, b)\n",
        "\n",
        "class Options(object):\n",
        "    '''Options used by LINE model.'''\n",
        "\n",
        "    def __init__(self):\n",
        "        # Model options.\n",
        "\n",
        "        # Embedding dimension.\n",
        "        self.emb_dim = FLAGS.embedding_size\n",
        "\n",
        "        # training text file.\n",
        "        self.train_data = FLAGS.train_data\n",
        "        self.label_data = FLAGS.label_data\n",
        "        self.walks_data = FLAGS.walks_data\n",
        "\n",
        "        # The initial learning rate.\n",
        "        self.learning_rate = FLAGS.learning_rate\n",
        "\n",
        "        # Number of samples to train.\n",
        "        self.samples_to_train = FLAGS.samples_to_train * 1000000\n",
        "\n",
        "        self.num_sampled = FLAGS.num_sampled\n",
        "        self.context_size = FLAGS.context_size\n",
        "\n",
        "        # Number of examples for one training step.\n",
        "        self.batch_size = FLAGS.batch_size\n",
        "\n",
        "        # Where to write out embeddings.\n",
        "        self.save_path = FLAGS.save_path\n",
        "        self.is_train = FLAGS.is_train\n",
        "\n",
        "\n",
        "class SNE(object):\n",
        "\n",
        "    def __init__(self, options, session):\n",
        "        self._options = options\n",
        "        self._session = session\n",
        "        self._vertex2id = {}\n",
        "        self._id2vertex = []\n",
        "        self._vid2degree = []\n",
        "        self._edge_source_id = []\n",
        "        self._edge_target_id = []\n",
        "        self._edge_weight = []\n",
        "        self._edge_sign = {}\n",
        "        self._alias = []\n",
        "        self._prob = []\n",
        "        self._edge_count = 0\n",
        "        self._read_data(self._options.train_data, self._options.label_data)\n",
        "        # self.InitAliasTable()\n",
        "        self.build_graph()\n",
        "\n",
        "    def _read_data(self, filename, label_path):\n",
        "        '''Read data from training file.'''\n",
        "        weight = 1.\n",
        "        for line in open(filename):\n",
        "            chunkes = line.strip().split()\n",
        "            assert len(chunkes) == 3\n",
        "            [name_v1, name_v2, sign] = chunkes\n",
        "            '''Add current vertex to dict if it is not included yet.'''\n",
        "            if name_v1 not in self._vertex2id:\n",
        "                self._vertex2id[name_v1] = len(self._vertex2id)\n",
        "                self._id2vertex.append(name_v1)\n",
        "                self._vid2degree.append(0)\n",
        "\n",
        "            if name_v2 not in self._vertex2id:\n",
        "                self._vertex2id[name_v2] = len(self._vertex2id)\n",
        "                self._id2vertex.append(name_v2)\n",
        "                self._vid2degree.append(0)\n",
        "\n",
        "            vid1 = self._vertex2id[name_v1]\n",
        "            self._vid2degree[vid1] += weight\n",
        "            self._edge_source_id.append(vid1)\n",
        "\n",
        "            vid2 = self._vertex2id[name_v2]\n",
        "            self._vid2degree[vid2] += weight\n",
        "            self._edge_target_id.append(vid2)\n",
        "\n",
        "            self._edge_sign[(vid1, vid2)] = (int(sign)+1)/2\n",
        "            self._edge_weight.append(weight)\n",
        "\n",
        "        self._edge_num = len(self._edge_weight)\n",
        "        self._options.vertex_size = len(self._id2vertex)\n",
        "        s_idx, t_idx, signs = [], [], []\n",
        "        for (s, t), y in self._edge_sign.items():\n",
        "            s_idx.append(s)\n",
        "            t_idx.append(t)\n",
        "            signs.append(y)\n",
        "        self._y = np.asarray(signs)\n",
        "        self._s_idx = np.asarray(s_idx)\n",
        "        self._t_idx = np.asarray(t_idx)\n",
        "        logging.info(\"Edge number : %d\" % (self._edge_num))\n",
        "        logging.info(\"Vertex number : %d\" % (self._options.vertex_size))\n",
        "\n",
        "        df = pd.read_csv(label_path, sep='\\t', header=None)\n",
        "        df = df.applymap(str)\n",
        "        self._node_labels = df.set_index(0)[1].to_dict()\n",
        "        assert len(self._node_labels) == len(self._id2vertex)\n",
        "\n",
        "\n",
        "    def make_instances(self, f_walks,):\n",
        "        with open(f_walks, 'r') as f:\n",
        "            corpus = f.readlines()\n",
        "        data = []\n",
        "        labels = []\n",
        "        signs = []\n",
        "        for sentence in corpus:\n",
        "            id_sentence = [self._vertex2id[str(node)] for node in sentence.split()]\n",
        "            for instance in zip(*(id_sentence[i:] for i in xrange(self._options.context_size + 1))):\n",
        "                sign = []\n",
        "                for u, v in pairwise(instance):\n",
        "                    if (u, v) in self._edge_sign:\n",
        "                        sign.append(self._edge_sign[(u, v)])\n",
        "                    elif (v, u) in self._edge_sign:\n",
        "                        sign.append(self._edge_sign[(v, u)])\n",
        "                    else:\n",
        "                        raise ValueError((u,v))\n",
        "                assert len(sign) == len(instance)-1\n",
        "                data.append(instance[:-1])\n",
        "                labels.append(instance[-1])\n",
        "                signs.append(sign)\n",
        "        logging.info(\"number of data %d\" %len(data))\n",
        "        return data, labels, signs\n",
        "\n",
        "\n",
        "    def forward(self, sources, targets, signs):\n",
        "        '''Build the graph for the forward pass.'''\n",
        "        opts = self._options\n",
        "\n",
        "        targets = tf.reshape(targets, [-1, 1])\n",
        "\n",
        "        # embedding:[vertex_size,emb_dim]\n",
        "        init_width = 0.5\n",
        "\n",
        "        emb_vertex = tf.Variable(tf.random_uniform([opts.vertex_size, opts.emb_dim], -init_width, init_width),\n",
        "                                 name='emb_vertex')\n",
        "        w = tf.Variable(tf.random_uniform([opts.vertex_size, opts.emb_dim], dtype=tf.float32), name='proj_w')\n",
        "        b = tf.Variable(tf.zeros([opts.vertex_size], dtype=tf.float32), name=\"proj_b\")\n",
        "\n",
        "        sign_weights = tf.Variable(tf.random_uniform([2, opts.emb_dim], -init_width, init_width), name='weights')\n",
        "\n",
        "        self._emb_vertex = emb_vertex\n",
        "        self._sign_w = sign_weights\n",
        "        self._emb_context = w\n",
        "        self._proj_b = b\n",
        "\n",
        "        self.global_step = tf.Variable(0, trainable=False, name='global_step', dtype=tf.int32)\n",
        "\n",
        "        example_emb = tf.nn.embedding_lookup(emb_vertex, sources)\n",
        "        weight = tf.nn.embedding_lookup(sign_weights, signs)\n",
        "\n",
        "        # vector bilinear\n",
        "        bilinear = tf.reduce_sum(tf.multiply(example_emb, weight), 1)\n",
        "\n",
        "        loss = tf.reduce_mean(tf.nn.sampled_softmax_loss(weights=w, biases=b, inputs=bilinear, labels=targets,\n",
        "                                                         num_sampled=opts.num_sampled, num_classes=opts.vertex_size))\n",
        "        return loss\n",
        "\n",
        "    def optimize(self, loss):\n",
        "        '''build the graph to optimize the loss function.'''\n",
        "        optimizer = tf.train.AdagradOptimizer(1.0)\n",
        "        train = optimizer.minimize(loss,\n",
        "                                   global_step=self.global_step,\n",
        "                                   gate_gradients=optimizer.GATE_NONE\n",
        "                                   )\n",
        "        self._train = train\n",
        "\n",
        "    def build_graph(self):\n",
        "        '''build the graph for the full model.'''\n",
        "        opts = self._options\n",
        "        sources = tf.placeholder(tf.int32, shape=[opts.batch_size, opts.context_size])\n",
        "        self._sources = sources\n",
        "        targets = tf.placeholder(tf.int64, shape=[opts.batch_size])\n",
        "        self._targets = targets\n",
        "        signs = tf.placeholder(tf.int32, shape=[opts.batch_size, opts.context_size])\n",
        "        self._signs = signs\n",
        "\n",
        "        loss = self.forward(sources, targets, signs)\n",
        "        self._loss = loss\n",
        "        self.optimize(loss)\n",
        "\n",
        "        # create a saver.\n",
        "        self.saver = tf.train.Saver()\n",
        "\n",
        "        # Initialize all variables.\n",
        "        tf.global_variables_initializer().run()\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        '''trian the model.'''\n",
        "        opts = self._options\n",
        "\n",
        "        # print training info.\n",
        "        loss_list = []\n",
        "        last_count, last_time = self._edge_count, time.time()\n",
        "        self._edge_count = 0\n",
        "        walks, target, signs = self.make_instances(f_walks=opts.walks_data)\n",
        "        n_train_batches = np.round(int(len(walks)/opts.batch_size))\n",
        "        print(\"Total batch number %d\" %n_train_batches)\n",
        "        for minibatch_index in xrange(n_train_batches):\n",
        "\n",
        "            _sources, _targets, _signs = walks[minibatch_index*opts.batch_size:(minibatch_index+1)*opts.batch_size], \\\n",
        "                                         target[minibatch_index*opts.batch_size:(minibatch_index+1)*opts.batch_size],\\\n",
        "                                         signs[minibatch_index*opts.batch_size:(minibatch_index+1)*opts.batch_size]\n",
        "            feed_dict = {self._sources: _sources, self._targets: _targets, self._signs: _signs}\n",
        "            (loss, _) = self._session.run([self._loss, self._train], feed_dict=feed_dict)\n",
        "            loss_list.append(loss)\n",
        "            self._edge_count += opts.batch_size\n",
        "            if self._edge_count % 2000 == 0:\n",
        "                now = time.time()\n",
        "                rate = (self._edge_count - last_count) / (now - last_time)\n",
        "                progress = 100 * (self._edge_count) / float(opts.samples_to_train)\n",
        "                last_time = now\n",
        "                last_count = self._edge_count\n",
        "                average_loss = np.mean(np.array(loss_list))\n",
        "                print(\"loss:%6.2f average loss: %f edges/sec:%8.0f%%\\r\" % (\n",
        "                    loss, average_loss, rate\n",
        "                ), end=\"\")\n",
        "                sys.stdout.flush()\n",
        "                loss_list = []\n",
        "            if self._edge_count >= opts.samples_to_train:\n",
        "                break\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "    def save_model(self,):\n",
        "        with open(os.path.join(self._options.save_path, \"lbl_wiki_edit_emb.pkl\"), 'wb') as f:\n",
        "            pickle.dump(self._emb_vertex.eval(), f)\n",
        "            pickle.dump(self._sign_w.eval(), f)\n",
        "            pickle.dump(self._emb_context.eval(), f)\n",
        "            pickle.dump(self._id2vertex, f)\n",
        "            pickle.dump(self._vertex2id, f)\n",
        "            pickle.dump(self._edge_source_id, f)\n",
        "            pickle.dump(self._edge_target_id, f)\n",
        "            pickle.dump(self._edge_sign, f)\n",
        "            pickle.dump(self._node_labels, f)\n",
        "\n",
        "\n",
        "def main(_):\n",
        "    logging.basicConfig(level=logging.INFO)\n",
        "    if not FLAGS.train_data:\n",
        "        logging.error('no train file.')\n",
        "        sys.exit(1)\n",
        "    opts = Options()\n",
        "    with tf.Graph().as_default(), tf.Session() as session:\n",
        "        model = SNE(opts, session)\n",
        "        if opts.is_train:\n",
        "            model.train()\n",
        "            model.save_model()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svpOU-vnjd-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "a7888f04-5423-4646-eda1-154a61d33e02"
      },
      "source": [
        "import _pickle as cPickle\n",
        "data=cPickle.load(open(\"lbl_wiki_edit_emb.pkl\",\"rb\"))\n",
        "print(data)\n",
        "output=open(\"write.txt\",\"w\")\n",
        "print(output)\n",
        "output.write(str(data))\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.5952519e-01 -4.2832044e-01 -2.1700029e-01 ...  5.6378400e-01\n",
            "  -2.2245166e-01  2.3024037e-01]\n",
            " [ 1.0013357e+00  4.4578829e-01 -1.7685185e-01 ...  7.3828441e-01\n",
            "  -2.0298287e-01 -2.2543053e-01]\n",
            " [-1.3066356e+00  3.3907467e-01 -2.6423222e-01 ...  1.0815867e-01\n",
            "  -4.5139533e-01 -2.1959138e-01]\n",
            " ...\n",
            " [-1.2542987e-01  2.5417608e-01 -5.0086850e-01 ... -8.9088666e-01\n",
            "  -5.4888617e-02 -1.6740407e-01]\n",
            " [-9.5513326e-01  7.1036853e-02  4.6955034e-01 ...  2.9774106e-04\n",
            "   6.6676414e-01  2.1581499e-01]\n",
            " [-7.2186881e-01 -1.4025319e-01  4.7376033e-02 ... -5.1219696e-01\n",
            "  -6.6446804e-02 -3.2500628e-01]]\n",
            "<_io.TextIOWrapper name='write.txt' mode='w' encoding='UTF-8'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "599"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}